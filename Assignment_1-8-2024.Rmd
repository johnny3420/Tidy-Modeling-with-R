---
title: "Assignment_1-8-2024"
output: 
  html_document: 
    keep_md: yes
date: "2024-01-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(modeldata)
library(tidyverse)
library(lubridate)
library(tidymodels)
data("Chicago")
Chicago
```

## 1.  Explore the data

Make a histogram of ridership.  What might be causing the two peaks.  Is there a predictor variable that can account for this (or that can be used to make a new variable to account for it)?  
I am not suggesting that you do regressions or plots on all variables at this time, rather that you think about what might have this kind of impact.  
If you need to make a new predictor variable, go ahead.

```{r}
ggplot(Chicago, aes(x = ridership)) +
  geom_histogram()
```

```{r}
library(lubridate)
Chicago <- Chicago %>%
  mutate(DoW = as.character(wday(ymd(date), label = T, abbr = F)),
         Weekend = ifelse(DoW == "Saturday" |DoW == "Sunday",
                          "Weekend",
                          "Weekday")) %>%
  select(-DoW)

ggplot(Chicago, aes(x = ridership, fill = Weekend)) +
  geom_density(alpha = 0.5)
```

## 2. Training and Test
Make an 80/20 train/test split.  Do you need to stratify over anything?
So that we are working on the same split, use `set.seed(010324)` in you code chunk

```{r}
set.seed(010324)
# Since there's 2 clear peaks of ridership we should stratify over ridership to make sure we get even amounts of samples across the two peaks
chicago_split <- initial_split(Chicago, prop = 0.80, strata = ridership)
chicago_train <- training(chicago_split)
chicago_test <- testing(chicago_split)
```

## 3. Workflow set
Let's compare the effectiveness  of the `temp` and `percip` [six] predictors.

### 3A
Use a workflow set (see chapter 7) to fit six models, each of which has your predictor from Q1 along with one of the following variables:
`temp_min`, `temp`, `temp_max`, `temp_change`, `percip`, `percip_max`
The formula for one of these would be something like `ridership ~ temp_min + Q1_predictor`.

```{r}
formulas <- list(
  ridership ~ temp_min + Weekend,
  ridership ~ temp + Weekend,
  ridership ~ temp_max + Weekend,
  ridership ~ temp_change + Weekend,
  ridership ~ percip + Weekend,
  ridership ~ percip_max + Weekend
)
lm_model <- linear_reg() %>% set_engine("lm")
rider_models <- workflow_set(preproc = formulas,
                             models = list(lm = lm_model)
                            )

rider_models <-
   rider_models %>%
   mutate(fit = map(info, ~ fit(.x$workflow[[1]], chicago_train)))
```

### 3B
Compare the model fits / predictors (this can be using any of the p-value of the predictor, R2, AIC, log-lik).  Don't worry about the test set, just compare goodness of fit when fit with the training set.

```{r}
rider_models %>%
  mutate(tidy=map(fit, tidy)) %>%
  unnest(tidy) %>%
  filter(grepl("temp|percip", term)) %>%
  arrange(p.value) %>%
  select(wflow_id, term, info, p.value)
```

## 4 Recipes
### 4A
Create a workflow recipe does the following:
* normalizes all weather and station predictors
* creates a set of PCs for the weather-related predictors, keeping enough PCs to explain 75% of the variance in the weather variables
* creates a second set of PCs for the station-related predictors, keeping enough PCs to explaining 75% of the variance in these variables
Hint: tidy(), prep(), and bake() methods for recipes may be helpful in examining what you have done.  The help file on recipe is good too.
Hint2: You can use various dplyr::select functions and regular expressions to avoid having to type out the variable names.  But as a fair-warning, it took me a lot longer to figure that out than it would have to just type then out.  (But next time it might be faster).  I can demo.

```{r}
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) %>%
  update_role(Austin:California, new_role = "station") %>%
  update_role(temp_min:weather_storm, new_role = "weather") %>%
  step_normalize(has_role("station"), has_role("weather")) %>%
  step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
  step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")

chicago_rec
```

### 4B
Use the recipe from 4A to fit a linear regression of ridership on the new PCs and all remaining predictors (i.e. those not used in making the PCs).  Use the training data.

```{r}
lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(chicago_rec)

lm_fit <- fit(lm_wflow, chicago_train)

tidy(lm_fit) %>% arrange(p.value)
```

### 4C
Use the fit from 4B to predict ridership in the test data.  Evaluate the predictions.

```{r}
chicago_no_home <- chicago_test %>%
  mutate(Home = Blackhawks_Home + Cubs_Home + Bulls_Home + Bears_Home + WhiteSox_Home,
         Home = ifelse(Home > 0, TRUE, FALSE)) %>%
  select(Home)
chicago_test_res <- predict(lm_fit, new_data = chicago_test %>% select(-ridership))
chicago_test_res <- bind_cols(chicago_test_res, chicago_test %>% select(ridership, Weekend), chicago_no_home)
```

```{r}
ggplot(chicago_test_res, aes(x = ridership, y = .pred, color = Home, shape = Weekend)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted ridership", x = "Ridership") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()
```

Few random spikes

```{r}
rmse(chicago_test_res, truth = ridership, estimate = .pred)
```
